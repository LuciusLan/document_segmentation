{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-16T12:52:32.923556Z","iopub.execute_input":"2022-04-16T12:52:32.923803Z","iopub.status.idle":"2022-04-16T12:52:32.928367Z","shell.execute_reply.started":"2022-04-16T12:52:32.923774Z","shell.execute_reply":"2022-04-16T12:52:32.927376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN_PATH = '/kaggle/input/feedback-prize-2021/train'\nTEST_PATH = '/kaggle/input/feedback-prize-2021/test'\nTRAIN_LABEL = '/kaggle/input/feedback-prize-2021/train.csv'\nSUBMISSION = '/kaggle/working/submission.csv'\nLABEL_2_ID = {'PAD':0, 'Claim': 1, 'Evidence': 2, 'Position': 3,\n              'Concluding Statement': 4, 'Lead': 5, 'Counterclaim': 6, 'Rebuttal': 7, 'non': 8}\nLABEL_BIO = {'PAD':0, 'B1': 1, 'I1': 2, 'B2': 3, 'I2': 4, 'B3': 5, 'I3': 6, 'B4': 7, 'I4': 8, 'B5': 9, 'I5': 10,\n             'B6': 11, 'I6': 12, 'B7': 13, 'I7': 14, 'O': 15}\nBOUNDARY_LABEL = {'PAD':0, 'B': 1, 'E': 2, 'O': 3}\nBOUNDARY_LABEL_UNIDIRECTION = {'PAD':0, 'B': 1, 'O': 3}\nTEST_SIZE = 0\nDEV_SIZE = 0.1\nMAX_LEN = 512\nNUM_LABELS = 16\nBATCH_SIZE = 6\nLEARNING_RATE = 5e-5\nNUM_EPOCH = 1\nLSTM_HIDDEN = 100\nBIAFFINE_DROPOUT = 0.5\nBASELINE = False\nLONGBERT = False \n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:52:35.346161Z","iopub.execute_input":"2022-04-16T12:52:35.346683Z","iopub.status.idle":"2022-04-16T12:52:35.35422Z","shell.execute_reply.started":"2022-04-16T12:52:35.346646Z","shell.execute_reply":"2022-04-16T12:52:35.353405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unicodedata import bidirectional\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.autograd import profiler\nfrom transformers import AutoModel\nfrom typing import Optional\n\nclass TModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        if LONGBERT:\n            self.transformer = AutoModel.from_pretrained(\n                pretrained_model_name_or_path=\"allenai/longformer-base-4096\", cache_dir=MODEL_CACHE_DIR, config=config)\n        else:\n            self.transformer = AutoModel.from_pretrained(\n                pretrained_model_name_or_path=\"/kaggle/input/robertabase/\", config=config)\n        self.dropout = nn.Dropout()\n        self.relu = nn.ReLU(True)\n        if BASELINE:\n            self.ner = nn.Linear(config.hidden_size, len(LABEL_BIO))\n        else:\n            self.boundary_encoder = nn.LSTM(bidirectional=True, input_size=config.hidden_size, hidden_size=LSTM_HIDDEN, batch_first=True)\n            self.boundary_decoder = nn.LSTM(bidirectional=False, input_size=LSTM_HIDDEN*2, hidden_size=LSTM_HIDDEN, batch_first=True)\n            self.boundary_biaffine = BoundaryBiaffine(LSTM_HIDDEN, LSTM_HIDDEN*2, len(BOUNDARY_LABEL_UNIDIRECTION))\n            #self.boundary_seg = BoundarySeg()\n            self.boundary_final0 = nn.Linear(config.hidden_size, LSTM_HIDDEN*2)\n            self.boundary_final1 = nn.Linear(LSTM_HIDDEN*2, LSTM_HIDDEN*2)\n            self.boundary_fc = nn.Linear(LSTM_HIDDEN*2, len(BOUNDARY_LABEL))\n\n            # No *2 since the boundary decoder can only be unidirectioal\n            self.seg_final0 = nn.Linear(config.hidden_size, LSTM_HIDDEN)\n            self.seg_final1 = nn.Linear(LSTM_HIDDEN, LSTM_HIDDEN)\n            self.boundary = nn.ModuleList([self.boundary_encoder, self.boundary_decoder, self.boundary_biaffine, self.boundary_final0, self.boundary_final1, self.boundary_fc])\n            self.type_lstm = nn.LSTM(bidirectional=True, input_size=config.hidden_size, hidden_size=LSTM_HIDDEN, batch_first=True)\n            self.type_final0 = nn.Linear(config.hidden_size, LSTM_HIDDEN*2)\n            self.type_final1 = nn.Linear(LSTM_HIDDEN*2, LSTM_HIDDEN*2)\n            self.type_fc = nn.Linear(LSTM_HIDDEN*2, len(LABEL_2_ID))\n            self.type_predict = nn.ModuleList([self.type_lstm, self.type_final0, self.type_final1, self.type_fc])\n            self.ner_final = nn.Linear(LSTM_HIDDEN*5+config.hidden_size, len(LABEL_BIO))\n            self.ner = nn.ModuleList([self.seg_final0, self.seg_final1, self.ner_final])\n        self.get_trigram = nn.Conv1d(LSTM_HIDDEN*2, LSTM_HIDDEN*2, 3, padding=1, bias=False)\n        self.get_trigram.weight = torch.nn.Parameter(torch.ones([LSTM_HIDDEN*2, LSTM_HIDDEN*2, 3]), requires_grad=False)\n        self.get_trigram.requires_grad_ = False\n        \n    \n    def forward(self,\n                input_ids=None,\n                attention_mask=None,\n                token_type_ids=None,\n                position_ids=None,\n                head_mask=None,\n                inputs_embeds=None,\n                output_attentions=None,\n                output_hidden_states=None,\n                return_dict=None,\n                ):\n        outputs = self.transformer(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        if BASELINE:\n            ner_result = self.ner(sequence_output)\n            return ner_result\n        else:\n            boundary_hidden = self.boundary_encoder(sequence_output)[0]\n            seg_result = self.get_trigram(boundary_hidden.transpose(1,2)).transpose(1,2)\n            seg_result = self.boundary_decoder(seg_result)[0]\n            seg_matrix = self.boundary_biaffine(seg_result, boundary_hidden)\n            #seg_result = F.softmax(self.boundary_biaffine(seg_result, boundary_hidden), dim=2)\n            #seg_result = self.boundary_seg(seg_result, boundary_hidden)\n            boundary_result = F.logsigmoid(self.boundary_final0(sequence_output)+self.boundary_final1(boundary_hidden)).mul(boundary_hidden)\n            type_hidden = self.type_lstm(sequence_output)[0]\n            type_result = F.logsigmoid(self.type_final0(sequence_output)+self.type_final1(type_hidden)).mul(type_hidden)\n            ner_result = F.logsigmoid(self.seg_final0(sequence_output)+self.seg_final1(seg_result)).mul(seg_result)\n            ner_result = self.ner_final(torch.cat([sequence_output, boundary_result, type_result, seg_result], dim=-1))\n            #del seg_result, boundary_result, type_result\n            #torch.cuda.empty_cache()\n            return ner_result, self.boundary_fc(boundary_hidden), self.type_fc(type_hidden), seg_matrix\n\n\nclass BoundarySeg(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, span_adjacency, bound_hidden):\n        with profiler.profile(with_stack=True) as p:\n            temp = []\n            for j in range(MAX_LEN):\n                j_sum = []\n                for i in range(j, MAX_LEN):\n                    result = torch.cat([bound_hidden[:, i], bound_hidden[:, j]], 1)\n                    result = result * span_adjacency[:, j, i]\n                    j_sum.append(result)\n                temp.append(torch.sum(torch.stack(j_sum, dim=0), dim=0))\n            final = torch.stack(temp, 1)\n        print(p.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\"))\n        return final\n\n\nclass PairwiseBilinear(nn.Module):\n    \"\"\" A bilinear module that deals with broadcasting for efficient memory usage.\n    Input: tensors of sizes (N x L1 x D1) and (N x L2 x D2)\n    Output: tensor of size (N x L1 x L2 x O)\"\"\"\n    def __init__(self, input1_size, input2_size, output_size, bias=True):\n        super().__init__()\n\n        self.input1_size = input1_size\n        self.input2_size = input2_size\n        self.output_size = output_size\n\n        self.weight = nn.Parameter(torch.zeros(input1_size, input2_size, output_size), requires_grad=True)\n        self.bias = nn.Parameter(torch.zeros(output_size), requires_grad=True) if bias else 0\n\n    def forward(self, input1, input2):\n        input1_size = list(input1.size())\n        input2_size = list(input2.size())\n        output_size = [input1_size[0], input1_size[1], input2_size[1], self.output_size]\n\n        # ((N x L1) x D1) * (D1 x (D2 x O)) -> (N x L1) x (D2 x O)\n        intermediate = torch.mm(input1.contiguous().view(-1, input1_size[-1]), self.weight.view(-1, self.input2_size * self.output_size))\n        # (N x L2 x D2) -> (N x D2 x L2)\n        input2 = input2.transpose(1, 2)\n        # (N x (L1 x O) x D2) * (N x D2 x L2) -> (N x (L1 x O) x L2)\n        output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)\n        # (N x (L1 x O) x L2) -> (N x L1 x L2 x O)\n        output = output.view(input1_size[0], input1_size[1], self.output_size, input2_size[1]).transpose(2, 3).contiguous()\n        # (N x L1 x L2 x O) + (O) -> (N x L1 x L2 x O)\n        output = output + self.bias\n\n        return output\n\nclass BoundaryBiaffine(nn.Module):\n    def __init__(self, input1_size, input2_size, output_size):\n        super().__init__()\n        self.W_bilin = PairwiseBilinear(input1_size, input2_size, output_size)\n        self.U = nn.Linear(input1_size, output_size)\n        self.V = nn.Linear(input2_size, output_size)\n\n    def forward(self, input1, input2):\n        # Changed from original pairwise biaffine, U is only on input1 (d_j) V is only on input2 (h_i^Bdy)\n        return self.W_bilin(input1, input2).add(self.U(input1).unsqueeze(2)).add(self.V(input2).unsqueeze(1))\n\nclass FocalLoss(torch.nn.Module):\n    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n    It is essentially an enhancement to cross entropy loss and is\n    useful for classification tasks when there is a large class imbalance.\n    x is expected to contain raw, unnormalized scores for each class.\n    y is expected to contain class labels.\n    Shape:\n        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n    \"\"\"\n\n    def __init__(self,\n                 alpha: Optional[torch.Tensor] = None,\n                 gamma: float = 0.,\n                 reduction: str = 'mean',\n                 ignore_index: int = -100):\n        \"\"\"Constructor.\n        Args:\n            alpha (Tensor, optional): Weights for each class. Defaults to None.\n            gamma (float, optional): A constant, as described in the paper.\n                Defaults to 0.\n            reduction (str, optional): 'mean', 'sum' or 'none'.\n                Defaults to 'mean'.\n            ignore_index (int, optional): class label to ignore.\n                Defaults to -100.\n        \"\"\"\n        if reduction not in ('mean', 'sum', 'none'):\n            raise ValueError(\n                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n\n        self.log_softmax = torch.nn.LogSoftmax(-1)\n        self.nll_loss = torch.nn.NLLLoss(\n            weight=alpha, reduction='none', ignore_index=ignore_index)\n\n    def __repr__(self):\n        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n        arg_vals = [self.__dict__[k] for k in arg_keys]\n        arg_strs = [f'{k}={v}' for k, v in zip(arg_keys, arg_vals)]\n        arg_str = ', '.join(arg_strs)\n        return f'{type(self).__name__}({arg_str})'\n\n    def forward(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n        if x.ndim > 2:\n            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n            c = x.shape[1]\n            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n            y = y.view(-1)\n\n        unignored_mask = y != self.ignore_index\n        y = y[unignored_mask]\n        if len(y) == 0:\n            return 0.\n        x = x[unignored_mask]\n\n        # compute weighted cross entropy term: -alpha * log(pt)\n        # (alpha is already part of self.nll_loss)\n        log_p = self.log_softmax(x)\n        ce = self.nll_loss(log_p, y)\n\n        # get true class column from each row\n        all_rows = torch.arange(len(x))\n        log_pt = log_p[all_rows, y]\n\n        # compute focal term: (1 - pt)^gamma\n        pt = log_pt.exp()\n        focal_term = (1 - pt)**self.gamma\n\n        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n        loss = focal_term * ce\n\n        if self.reduction == 'mean':\n            loss = loss.mean()\n        elif self.reduction == 'sum':\n            loss = loss.sum()\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:52:46.815239Z","iopub.execute_input":"2022-04-16T12:52:46.815495Z","iopub.status.idle":"2022-04-16T12:52:48.589639Z","shell.execute_reply.started":"2022-04-16T12:52:46.815466Z","shell.execute_reply":"2022-04-16T12:52:48.588945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from posixpath import split\nimport pandas as pd\nimport nltk\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, Dataset\n\nimport re\nimport itertools\nimport six\n\n# sent_tok = nltk.data.load(f\"tokenizers/punkt/English.pickle\")\nre_bos = re.compile(r'^\\s?\\W?(?:(?:[A-Z]{1}[a-z]+)|(?:I))\\s?[a-z]*')\nre_eos = re.compile(r'[?\\.!]\\'?\\\"?\\s*$')\n\n\ndef preprocessing_train(labels: pd.DataFrame, raw_text: str, tokenizer) -> \"tuple[list]\":\n    \"\"\"\n    Tokenization for training. Insert [NP] tokens at new paragraph\n\n    Args:\n        labels: the DataFrame containing label information.\n        raw_text: the raw text input as string\n\n    Returns:\n        new_segements: list of encoded tokenized inputs, organized in segments\n        discourse_type: list of segments' type\n        subword_mask: list of subword masks (for post-processing)\n    \"\"\"\n    err = False\n    new_segements = []\n    prev_end = -1\n    prev_shift = 0\n    prev_label = -1\n    subword_mask = []\n    seg_labels = []\n    raw_text = raw_text.replace('\\xa0', ' ')\n    raw_text = raw_text.replace('Â', '')\n    prev_eos = True\n    splitted = re.sub('\\n+', ' [NP]', raw_text).split(' ')\n    for _, segment in labels.iterrows():\n        seg_ids = []\n        positions = segment['predictionstring'].split(' ')\n        positions = [int(e) for e in positions]\n        start = positions[0]\n        end = positions[-1]\n        # Find is there any text before current discourse start and previous discourse end\n        # Or any text before the first discourse start\n\n        # Find if there is still any span before current discourse start and prev discourse end\n        if prev_end < start or (prev_end == -1 and start != 0):\n            if prev_end == -1:\n                hold_seg = splitted[: start]\n            else:\n                hold_seg = splitted[prev_end: start]\n            hold_seg = ' '.join(hold_seg)\n            hold_seg = re.sub('\\n+', ' [NP] ', hold_seg)\n            temp_sents = nltk.tokenize.sent_tokenize(hold_seg)\n            temp_ids = []\n            temp_label = []\n            for i, sent in enumerate(temp_sents):\n                tokenized_hold = tokenizer(sent)\n                hold_seg_ids = tokenized_hold['input_ids']\n                # Remove [CLS] or [SEP] token if segment start is not start of new sentence\n                # or segment end not end of sentence\n                # If previous segment ends with EOS, assign bos to current segment\n                if (not re_bos.search(sent)) and (not prev_eos):\n                    hold_seg_ids = hold_seg_ids[1:]\n                if not re_eos.search(sent):\n                    hold_seg_ids = hold_seg_ids[:-1]\n                    prev_eos = False\n                else:\n                    prev_eos = True\n                if i == 0 and prev_shift == len(sent.split(' ')):\n                    temp_label.extend([prev_label]*len(hold_seg_ids))\n                else:\n                    temp_label.extend([8]*len(hold_seg_ids))\n                temp_ids.extend(hold_seg_ids)\n\n            if len(temp_ids) != 0 and len(temp_label) != 0:\n                new_segements.append(temp_ids)\n                seg_labels.append(temp_label)\n\n        seg = splitted[start:end+1]\n        seg = ' '.join(seg)\n        # Insert special token for New Paragraph (strong indicator for boundary)\n        seg = re.sub('\\n+', ' [NP] ', seg)\n\n        temp_sents = nltk.tokenize.sent_tokenize(seg)\n        temp_ids = []\n        temp_label = []\n        for sent in temp_sents:\n            tokenized_sent = tokenizer(sent)\n            a='For example, the text states, ¨A thick atmosphere o'\n            seg_ids = tokenized_sent['input_ids']\n            # Remove [CLS] or [SEP] token if segment start is not start of new sentence\n            # or segment end not end of sentence\n            if (not re_bos.search(sent) and not prev_eos) and sent != '[NP]':\n                seg_ids = seg_ids[1:]\n            if (not re_eos.search(sent)) and sent != '[NP]':\n                seg_ids = seg_ids[:-1]\n                prev_eos = False\n            else:\n                prev_eos = True\n            current_seg_label = [\n                LABEL_2_ID[segment['discourse_type']]]*len(seg_ids)\n            temp_label.extend(current_seg_label)\n            temp_ids.extend(seg_ids)\n        if len(temp_ids) != 0 and len(temp_label) != 0:\n            seg_labels.append(temp_label)\n            new_segements.append(temp_ids)\n        if len(positions) < len(segment['discourse_text'].split(' ')) and segment['discourse_text'].split(' ') != '':\n            prev_shift = len(segment['discourse_text'].split(' ')) - len(positions)\n            prev_label = current_seg_label[0]\n        else:\n            prev_shift = 0\n        prev_end = end+1\n\n    # Find is there any text after the last discourse end\n    if end+1 < len(splitted):\n        hold_seg_ids = []\n        hold_seg = splitted[end+1:]\n        hold_seg = [e for e in hold_seg if e != '']\n        if len(hold_seg) > 0:\n            hold_seg = ' '.join(hold_seg)\n            hold_seg = re.sub('\\n+', ' [NP] ', hold_seg)\n            temp_sents = nltk.tokenize.sent_tokenize(hold_seg)\n            temp_ids = []\n            temp_label = []\n            for i, sent in enumerate(temp_sents):\n                tokenized_hold = tokenizer(sent)\n                hold_seg_ids = tokenized_hold['input_ids']\n                # Remove [CLS] or [SEP] token if segment start is not start of new sentence\n                # or segment end not end of sentence\n                if not re_bos.search(sent) and not prev_eos:\n                    hold_seg_ids = hold_seg_ids[1:]\n                if not re_eos.search(sent):\n                    hold_seg_ids = hold_seg_ids[:-1]\n                    prev_eos = False\n                else:\n                    prev_eos = True\n                if i == 0 and prev_shift == len(sent.split(' ')):\n                    temp_label.extend([prev_label]*len(hold_seg_ids))\n                else:\n                    temp_label.extend([8]*len(hold_seg_ids))\n                temp_ids.extend(hold_seg_ids)\n            if len(temp_ids) != 0 and len(temp_label) != 0:\n                new_segements.append(temp_ids)\n                seg_labels.append(temp_label)\n\n\n    tokenized = []\n    for e in new_segements:\n        tokenized.extend(tokenizer.convert_ids_to_tokens(e))\n    \n    tok_counter = 0\n    hold = ''\n    for i, tok in enumerate(tokenized):\n        # Assign special token subword mask -1\n        # '[NP]' needs to be treated differently, as part of word\n        if tok in ['<s>', '</s>']:\n            subword_mask.append(-1)\n            continue\n        # RoBERTa and Longformer tokenizer use this char to denote start of new word\n        if tok.startswith('Ġ'):\n            tok = tok[1:]\n        #if tok in ['Â']:\n        #    print()\n        #    continue\n        # If BERT token matches simple split token, append position as subword mask\n        if splitted[tok_counter] == tok:\n            subword_mask.append(tok_counter)\n            tok_counter+=1\n            hold = ''\n        # Else, combine the next BERT token until there is a match (e.g.: original: \"Abcdefgh\", BERT: \"Abc\", \"def\", \"gh\")\n        # each subword of full word are assigned same full word position\n        else:\n            hold+=tok\n            subword_mask.append(tok_counter)\n            if splitted[tok_counter] == hold:\n                hold = ''\n                tok_counter+=1\n        # if combined token length larger than 50, most likely something wrong happened\n        if len(hold)>50:\n            err = True\n    assert len(subword_mask) == len(list(itertools.chain.from_iterable(new_segements))) == len(list(itertools.chain.from_iterable(seg_labels))), \"Length of ids/labels/subword_mask mismatch\"\n    return new_segements, seg_labels, subword_mask, err\n\n\ndef preprocessing_test(raw_text: str, tokenizer) -> \"tuple[list]\":\n    \"\"\"\n    Tokenization or testing (without ground truth), simply tokenize and output subword mask\n    Need to take care of [NP] tokens when decoding\n    \"\"\"\n    ids = []\n    subword_mask = []\n    err = False\n    raw_text = raw_text.replace('\\xa0', ' ')\n    raw_text = re.sub('\\n+', ' [NP] ', raw_text)\n    temp_sents = nltk.tokenize.sent_tokenize(raw_text)\n    for sent in temp_sents:\n        tokenized_sent = tokenizer(sent)\n        ids.extend(tokenized_sent['input_ids'])\n\n    tokenized = tokenizer.convert_ids_to_tokens(ids)\n    splitted = re.sub('\\n+', ' [NP] ', raw_text).split(' ')\n    tok_counter = 0\n    hold = ''\n    for i, tok in enumerate(tokenized):\n        # Assign special token subword mask -1\n        # '[NP]' needs to be treated differently, as part of word\n        if tok in ['<s>', '</s>']:\n            subword_mask.append(-1)\n            continue\n        # RoBERTa and Longformer tokenizer use this char to denote start of new word\n        if tok.startswith('Ġ'):\n            tok = tok[1:]\n        # If BERT token matches simple split token, append position as subword mask\n        if splitted[tok_counter] == tok:\n            subword_mask.append(tok_counter)\n            tok_counter+=1\n            hold = ''\n        # Else, combine the next BERT token until there is a match (e.g.: original: \"Abcdefgh\", BERT: \"Abc\", \"def\", \"gh\")\n        # each subword of full word are assigned same full word position\n        else:\n            hold+=tok\n            subword_mask.append(tok_counter)\n            if splitted[tok_counter] == hold:\n                hold = ''\n                tok_counter+=1\n        # if combined token length larger than 50, most likely something wrong happened\n        if len(hold)>50:\n            err = True\n    return ids, subword_mask, err\n\n\nclass SlidingWindowFeature():\n    def __init__(self, doc_id, input_ids, labels_type, labels_bio, labels_boundary, subword_masks, cls_pos, sliding_window, tokenizer=None) -> None:\n        self.doc_id = doc_id\n        self.tokenizer = tokenizer\n        self.cls_pos = cls_pos\n        self.sliding_window = sliding_window\n        if sliding_window is not None:\n            self.input_ids = [input_ids[start:end]\n                              for start, end in sliding_window]\n            self.subword_masks = [subword_masks[start:end]\n                                  for start, end in sliding_window]\n            self.labels_type = [labels_type[start:end]\n                                for start, end in sliding_window]\n            self.labels_bio = [labels_bio[start:end]\n                               for start, end in sliding_window]\n            self.labels_boundary = [labels_boundary[start:end]\n                                    for start, end in sliding_window]\n            self.num_windows = len(sliding_window)\n        else:\n            self.input_ids = [input_ids]\n            self.subword_masks = [subword_masks]\n            self.labels_type = [labels_type]\n            self.labels_bio = [labels_bio]\n            self.labels_boundary = [labels_boundary]\n            self.sliding_window = [[0, len(input_ids)]]\n            self.num_windows = 1\n\n\nclass SlidingWindowFeatureTest():\n    def __init__(self, doc_id, input_ids, subword_masks, cls_pos, sliding_window, tokenizer=None) -> None:\n        self.doc_id = doc_id\n        self.tokenizer = tokenizer\n        self.cls_pos = cls_pos\n        self.sliding_window = sliding_window\n        if sliding_window is not None:\n            self.input_ids = [input_ids[start:end]\n                              for start, end in sliding_window]\n            self.subword_masks = [subword_masks[start:end]\n                                  for start, end in sliding_window]\n            self.num_windows = len(sliding_window)\n        else:\n            self.input_ids = [input_ids]\n            self.subword_masks = [subword_masks]\n            self.sliding_window = [[0, len(input_ids)]]\n            self.num_windows = 1\n\n\nclass DocFeature():\n    def __init__(self, doc_id: str, raw_text: str, train_or_test: str, seg_labels=None, tokenizer=None) -> None:\n        self.doc_id = doc_id\n        self.tokenizer = tokenizer\n        if train_or_test == 'train':\n            self.input_ids, self.seg_labels, self.subword_masks, self.err = preprocessing_train(\n                labels=seg_labels, raw_text=raw_text, tokenizer=tokenizer)\n            #self.labels = [[label]*len(seg) for seg, label in zip(self.input_ids, label_ids)]\n            self.labels_bio = [self.convert_label_to_bio(label, len(\n                seg)) for seg, label in zip(self.input_ids, self.seg_labels)]\n            self.labels_bio = list(\n                itertools.chain.from_iterable(self.labels_bio))\n            self.labels = list(itertools.chain.from_iterable(self.seg_labels))\n            self.input_ids = list(\n                itertools.chain.from_iterable(self.input_ids))\n            self.boundary_pos = self.get_boundary_pos()\n            self.cls_pos = [index for index, element in enumerate(\n                self.input_ids) if element == tokenizer.cls_token_id]\n            self.count = self.get_sent_level_label()\n            self.boundary_label = self.convert_label_to_bound()\n            self.sliding_window = self.create_sliding_window_train()\n        elif train_or_test == 'test':\n            self.input_ids, self.subword_masks, self.err = preprocessing_test(\n                raw_text, tokenizer=tokenizer)\n            self.cls_pos = [index for index, element in enumerate(\n                self.input_ids) if element == tokenizer.cls_token_id]\n            self.sliding_window = self.create_sliding_window_test()\n        else:\n            raise NameError('Should be either train/test')\n\n    def convert_label_to_bio(self, label, seq_len):\n        if label[0] != 8:\n            temp = [LABEL_BIO[f'I{label[0]}']]*seq_len\n            temp[0] = LABEL_BIO[f'B{label[0]}']\n        else:\n            temp = [LABEL_BIO['O']]*seq_len\n        return temp\n\n    def convert_label_to_bound(self):\n        bound = []\n        for i, e in enumerate(self.labels_bio):\n            if e in [1, 3, 5, 7, 9, 11, 13]:\n                bound.append(1)\n                if i == 0:\n                    pass\n                else:\n                    bound[-2] = 2\n            elif e == 0:\n                bound.append(0)\n            else:\n                bound.append(3)\n        return bound\n\n    def get_sent_level_label(self):\n        prev_cls = 0\n        labels = list(itertools.chain.from_iterable(self.seg_labels))\n        count = 0\n        for pos in self.cls_pos:\n            distinct = set(labels[prev_cls:pos])\n            if (8 in distinct and len(distinct) > 2) or (8 not in distinct and len(distinct) > 1):\n                count += 1\n            prev_cls = pos\n        return count\n\n    def get_boundary_pos(self):\n        boundary = []\n        prev = 0\n        for seg in self.seg_labels:\n            boundary.append(len(seg)+prev)\n            prev = len(seg) + prev\n        return boundary\n\n    def create_sliding_window_train(self):\n        if len(self.input_ids) <= MAX_LEN:\n            return SlidingWindowFeature(doc_id=self.doc_id, input_ids=self.input_ids, labels_type=self.labels, labels_bio=self.labels_bio,\n                                        labels_boundary=self.boundary_label, subword_masks=self.subword_masks, cls_pos=self.cls_pos, sliding_window=None)\n        else:\n            # Create intersection of boundary pos list and cls token pos list, as we can only create slice on cls token, not any boundary\n            bound_cls_pos = list(\n                set(self.boundary_pos).intersection(set(self.cls_pos)))\n            bound_cls_pos.append(len(self.input_ids))\n            bound_cls_pos.sort()\n            slice_pos_list = []\n            slice_start = 0\n            slice_end = -1\n            # For the case that last candidate boundary is less than MAX_LEN: slice there.\n            if max(bound_cls_pos) < MAX_LEN:\n                slice_pos_list.append([0, bound_cls_pos[-1]])\n                if len(bound_cls_pos) > 1:\n                    slice_pos_list.append(\n                        [bound_cls_pos[-2]], len(self.input_ids))\n                else:\n                    print()\n            for pos in bound_cls_pos:\n                if (pos - slice_start) > MAX_LEN:\n                    # When the two adjacent boundary pos having distance larger than MAX_LEN, or the first boundary is already more than MAX_LEN\n                    if (slice_end == -1 or slice_end == slice_start) or (bound_cls_pos.index(slice_end) == 0):\n                        prev = 0\n                        for i, idx in enumerate(self.cls_pos[self.cls_pos.index(slice_start):]):\n                            if idx > MAX_LEN:\n                                break\n                            prev = idx\n                        slice_end = prev\n                        slice_pos_list.append([slice_start, slice_end])\n                        try:\n                            slice_start = self.cls_pos[i-2]\n                        except IndexError:\n                            slice_start = self.cls_pos[i-1]\n                        if slice_end in bound_cls_pos:\n                            if bound_cls_pos.index(slice_end) == 0:\n                                print()\n                    # Normal case, finding the n'th boundary having distance > MAX_LEN with slice_start\n                    # Make the n-1'th boundary become slice_end\n                    else:\n                        # When the n-1'th boundary is having distance with current slice start > MAX_LEN\n                        # Just find the first sentence boundary within it that has distacne < MAX_LEN\n                        if slice_end-slice_start > MAX_LEN:\n                            for idx in self.cls_pos[self.cls_pos.index(slice_start):]:\n                                if slice_end-idx <= MAX_LEN:\n                                    break\n                            slice_start = idx\n                        # If the n-1'th boundary is too short, pick a sentence boundary after it and before the next slice start\n                        # But skip when reaching the end of document, as there would no more boundary after it\n                        if slice_end-slice_start < 150 and pos != bound_cls_pos[-1]:\n                            next_start = bound_cls_pos[bound_cls_pos.index(\n                                slice_end)+1]\n                            if next_start - slice_start <= MAX_LEN:\n                                candidate_end = self.cls_pos[self.cls_pos.index(\n                                    slice_end): self.cls_pos.index(next_start)]\n                                slice_end = candidate_end[len(\n                                    candidate_end)-len(candidate_end) // 3]\n                            else:\n                                candidate_end = self.cls_pos[self.cls_pos.index(\n                                    slice_end): self.cls_pos.index(next_start)]\n                                for idx in candidate_end:\n                                    if idx-slice_start > MAX_LEN:\n                                        break\n                                    slice_end = idx\n                            slice_pos_list.append([slice_start, slice_end])\n                            slice_start = candidate_end[len(\n                                candidate_end) // 3]\n                        else:\n                            se_index = bound_cls_pos.index(slice_end)\n                            slice_pos_list.append([slice_start, slice_end])\n                            slice_start = bound_cls_pos[:se_index][-1]\n                slice_end = pos\n            if slice_pos_list[-1][1] != len(self.input_ids):\n                if slice_start != slice_pos_list[-1][0]:\n                    if len(self.input_ids) - slice_start:\n                        for idx in self.cls_pos[self.cls_pos.index(slice_start):]:\n                            if slice_end-idx <= MAX_LEN:\n                                break\n                        slice_start = idx\n                    slice_pos_list.append([slice_start, len(self.input_ids)])\n                else:\n                    for idx in self.cls_pos[self.cls_pos.index(slice_start):]:\n                        if slice_end-idx <= MAX_LEN:\n                            break\n                    slice_start = idx\n                    slice_pos_list.append([slice_start, len(self.input_ids)])\n            return SlidingWindowFeature(doc_id=self.doc_id, input_ids=self.input_ids, labels_type=self.labels, labels_bio=self.labels_bio,\n                                        labels_boundary=self.boundary_label, subword_masks=self.subword_masks, cls_pos=self.cls_pos, sliding_window=slice_pos_list)\n\n    def create_sliding_window_test(self):\n        if len(self.input_ids) <= MAX_LEN:\n            return SlidingWindowFeatureTest(doc_id=self.doc_id, input_ids=self.input_ids, subword_masks=self.subword_masks, cls_pos=self.cls_pos, sliding_window=None)\n        else:\n            slice_pos_list = []\n            slice_start = 0\n            slice_end = -1\n            if len(self.cls_pos) == 1:\n                return SlidingWindowFeatureTest(doc_id=self.doc_id, input_ids=self.input_ids, subword_masks=self.subword_masks, cls_pos=self.cls_pos, sliding_window=[[0, MAX_LEN]])\n            if max(self.cls_pos) <= MAX_LEN:\n                slice_pos_list.append([0, self.cls_pos[-1]])\n                try:\n                    slice_pos_list.append(\n                        [self.cls_pos[-4], len(self.input_ids)])\n                except IndexError:\n                    slice_pos_list.append(\n                        [self.cls_pos[-2], len(self.input_ids)])\n            else:\n                for i, pos in enumerate(self.cls_pos):\n                    if (pos-slice_start) > MAX_LEN:\n                        slice_pos_list.append([slice_start, slice_end])\n                        se_index = self.cls_pos.index(slice_end)\n                        ss_index = self.cls_pos.index(slice_start)\n                        temp = self.cls_pos[ss_index:se_index]\n                        if len(temp) > 2:\n                            slice_start = temp[len(temp) - (len(temp)//3)]\n                        else:\n                            slice_start = temp[-1]\n                    slice_end = pos\n                    if i == len(self.cls_pos)-1:\n                        slice_pos_list.append([slice_start, slice_end])\n                        se_index = self.cls_pos.index(slice_end)\n                        ss_index = self.cls_pos.index(slice_start)\n                        temp = self.cls_pos[ss_index:se_index]\n                        if len(temp) > 2:\n                            slice_start = temp[len(temp) - (len(temp)//3)]\n                        else:\n                            slice_start = temp[-1]\n                if slice_pos_list[-1][1] != len(self.input_ids):\n                    if slice_start != slice_pos_list[-1][0]:\n                        if len(self.input_ids) - slice_start:\n                            for idx in self.cls_pos[self.cls_pos.index(slice_start):]:\n                                if slice_end-idx <= MAX_LEN:\n                                    break\n                            slice_start = idx\n                        slice_pos_list.append(\n                            [slice_start, len(self.input_ids)])\n            return SlidingWindowFeatureTest(doc_id=self.doc_id, input_ids=self.input_ids, subword_masks=self.subword_masks, cls_pos=self.cls_pos, sliding_window=slice_pos_list)\n\n\ndef pad_sequences(sequences, maxlen=None, dtype='int32',\n                  padding='pre', truncating='pre', value=0.):\n    \"\"\"\n    \"\"\"\n    if not hasattr(sequences, '__len__'):\n        raise ValueError('`sequences` must be iterable.')\n    num_samples = len(sequences)\n\n    lengths = []\n    sample_shape = ()\n    flag = True\n\n    # take the sample shape from the first non empty sequence\n    # checking for consistency in the main loop below.\n\n    for x in sequences:\n        try:\n            lengths.append(len(x))\n            if flag and len(x):\n                sample_shape = np.asarray(x).shape[1:]\n                flag = False\n        except TypeError:\n            raise ValueError('`sequences` must be a list of iterables. '\n                             'Found non-iterable: ' + str(x))\n\n    if maxlen is None:\n        maxlen = np.max(lengths)\n\n    is_dtype_str = np.issubdtype(\n        dtype, np.str_) or np.issubdtype(dtype, np.unicode_)\n    if isinstance(value, six.string_types) and dtype != object and not is_dtype_str:\n        raise ValueError(\"`dtype` {} is not compatible with `value`'s type: {}\\n\"\n                         \"You should set `dtype=object` for variable length strings.\"\n                         .format(dtype, type(value)))\n\n    x = np.full((num_samples, maxlen) + sample_shape, value, dtype=dtype)\n    for idx, s in enumerate(sequences):\n        if not len(s):\n            continue  # empty list/array was found\n        if truncating == 'pre':\n            trunc = s[-maxlen:]\n        elif truncating == 'post':\n            trunc = s[:maxlen]\n        else:\n            raise ValueError('Truncating type \"%s\" '\n                             'not understood' % truncating)\n\n        # check `trunc` has expected shape\n        trunc = np.asarray(trunc, dtype=dtype)\n        if trunc.shape[1:] != sample_shape:\n            raise ValueError('Shape of sample %s of sequence at position %s '\n                             'is different from expected shape %s' %\n                             (trunc.shape[1:], idx, sample_shape))\n\n        if padding == 'post':\n            x[idx, :len(trunc)] = trunc\n        elif padding == 'pre':\n            x[idx, -len(trunc):] = trunc\n        else:\n            raise ValueError('Padding type \"%s\" not understood' % padding)\n    return x\n\n\ndef create_tensor_ds(features: \"list[DocFeature]\") -> TensorDataset:\n    input_ids = []\n    labels_bio = []\n    labels_boundary = []\n    labels_type = []\n    attention_masks = []\n    subword_masks = []\n    cls_pos = []\n    for feat in features:\n        input_ids.append(feat.input_ids)\n        labels_bio.append(feat.labels_bio)\n        labels_boundary.append(feat.boundary_label)\n        labels_type.append(feat.labels)\n        attention_masks.append([1]*len(feat.input_ids))\n        subword_masks.append(feat.subword_masks)\n        cls_pos.append(feat.cls_pos)\n    input_ids = pad_sequences(input_ids,\n                              maxlen=MAX_LEN, value=0, padding=\"post\",\n                              dtype=\"long\", truncating=\"post\").tolist()\n    input_ids = torch.LongTensor(input_ids)\n    labels_bio = pad_sequences(labels_bio,\n                           maxlen=MAX_LEN, value=0, padding=\"post\",\n                           dtype=\"long\", truncating=\"post\").tolist()\n    labels_bio = torch.LongTensor(labels_bio)\n    labels_boundary = pad_sequences(labels_boundary,\n                           maxlen=MAX_LEN, value=0, padding=\"post\",\n                           dtype=\"long\", truncating=\"post\").tolist()\n    labels_boundary = torch.LongTensor(labels_boundary)\n    labels_type = pad_sequences(labels_type,\n                           maxlen=MAX_LEN, value=0, padding=\"post\",\n                           dtype=\"long\", truncating=\"post\").tolist()\n    labels_type = torch.LongTensor(labels_type)\n    attention_masks = pad_sequences(attention_masks,\n                                    maxlen=MAX_LEN, value=0, padding=\"post\",\n                                    dtype=\"long\", truncating=\"post\").tolist()\n    attention_masks = torch.LongTensor(attention_masks)\n    subword_masks = pad_sequences(subword_masks,\n                                  maxlen=MAX_LEN, value=0, padding=\"post\",\n                                  dtype=\"long\", truncating=\"post\").tolist()\n    subword_masks = torch.LongTensor(subword_masks)\n    return TensorDataset(input_ids, labels_type, labels_bio, labels_boundary, attention_masks, subword_masks)\n\n\nclass SlidingWindowDataset(Dataset):\n    def __init__(self, input_ids: torch.Tensor,  labels_type: torch.Tensor, labels_bio: torch.Tensor, labels_boundary: torch.Tensor, attention_masks: torch.Tensor,\n                 subword_masks: torch.Tensor, cls_pos: list, sliding_window_pos: \"list[list]\") -> None:\n        self.input_ids = input_ids\n        self.labels_type = labels_type\n        self.labels_bio = labels_bio\n        self.labels_boundary = labels_boundary\n        self.attention_masks = attention_masks\n        self.subword_masks = subword_masks\n        self.cls_pos = cls_pos\n        self.sliding_window_pos = sliding_window_pos\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.labels_type[idx], self.labels_bio[idx], self.labels_boundary[idx], self.attention_masks[idx], self.subword_masks[idx], self.cls_pos[idx], self.sliding_window_pos[idx]\n\n\nclass SlidingWindowDatasetTest(Dataset):\n    def __init__(self, input_ids: torch.Tensor, attention_masks: torch.Tensor, subword_masks: torch.Tensor,\n                 cls_pos: list, sliding_window_pos: \"list[list]\") -> None:\n        self.input_ids = input_ids\n        self.attention_masks = attention_masks\n        self.subword_masks = subword_masks\n        self.cls_pos = cls_pos\n        self.sliding_window_pos = sliding_window_pos\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attention_masks[idx], self.subword_masks[idx], self.cls_pos[idx], self.sliding_window_pos[idx]\n\n\ndef create_tensor_ds_sliding_window(features: \"list[DocFeature]\") -> TensorDataset:\n    c = 0\n    input_ids = []\n    labels_bio = []\n    labels_type = []\n    labels_boundary = []\n    attention_masks = []\n    subword_masks = []\n    cls_pos = []\n    sliding_window_pos = []\n    for feat in features:\n        for i in range(feat.sliding_window.num_windows):\n            # If the document contains no puncutation at all... No way but just delete it\n            if len(feat.cls_pos) == 1:\n                continue\n            input_ids.append(feat.sliding_window.input_ids[i])\n            labels_bio.append(feat.sliding_window.labels_bio[i])\n            labels_boundary.append(feat.sliding_window.labels_boundary[i])\n            labels_type.append(feat.sliding_window.labels_type[i])\n            attention_masks.append([1]*len(feat.sliding_window.input_ids[i]))\n            subword_masks.append(feat.sliding_window.subword_masks[i])\n            cls_pos.append(feat.sliding_window.cls_pos)\n            sliding_window_pos.append(\n                [feat.sliding_window.sliding_window, feat.doc_id])\n            if len(feat.sliding_window.input_ids[i]) > MAX_LEN:\n                c += 1\n            if feat.sliding_window.sliding_window[i][0] == feat.sliding_window.sliding_window[i][1]:\n                print()\n            if i > 0 and feat.sliding_window.sliding_window[i][0] == feat.sliding_window.sliding_window[i-1][1]:\n                c += 1\n    input_ids = pad_sequences(input_ids,\n                              maxlen=MAX_LEN, value=0, padding=\"post\",\n                              dtype=\"long\", truncating=\"post\").tolist()\n    input_ids = torch.LongTensor(input_ids)\n    labels_type = pad_sequences(labels_type,\n                                maxlen=MAX_LEN, value=0, padding=\"post\",\n                                dtype=\"long\", truncating=\"post\").tolist()\n    labels_type = torch.LongTensor(labels_type)\n    labels_bio = pad_sequences(labels_bio,\n                               maxlen=MAX_LEN, value=0, padding=\"post\",\n                               dtype=\"long\", truncating=\"post\").tolist()\n    labels_bio = torch.LongTensor(labels_bio)\n    labels_boundary = pad_sequences(labels_boundary,\n                                    maxlen=MAX_LEN, value=0, padding=\"post\",\n                                    dtype=\"long\", truncating=\"post\").tolist()\n    labels_boundary = torch.LongTensor(labels_boundary)\n    attention_masks = pad_sequences(attention_masks,\n                                    maxlen=MAX_LEN, value=0, padding=\"post\",\n                                    dtype=\"long\", truncating=\"post\").tolist()\n    attention_masks = torch.LongTensor(attention_masks)\n    subword_masks = pad_sequences(subword_masks,\n                                  maxlen=MAX_LEN, value=0, padding=\"post\",\n                                  dtype=\"long\", truncating=\"post\").tolist()\n    subword_masks = torch.LongTensor(subword_masks)\n    return SlidingWindowDataset(input_ids, labels_type, labels_bio, labels_boundary, attention_masks, subword_masks, cls_pos, sliding_window_pos)\n\n\ndef create_tensor_ds_sliding_window_test(features: \"list[DocFeature]\") -> TensorDataset:\n    c = 0\n    input_ids = []\n    attention_masks = []\n    subword_masks = []\n    cls_pos = []\n    sliding_window_pos = []\n    for feat in features:\n        for i in range(feat.sliding_window.num_windows):\n            # If the document contains no puncutation at all... No way but just delete it\n            if len(feat.cls_pos) == 1:\n                continue\n            input_ids.append(feat.sliding_window.input_ids[i])\n            attention_masks.append([1]*len(feat.sliding_window.input_ids[i]))\n            subword_masks.append(feat.sliding_window.subword_masks[i])\n            cls_pos.append(feat.sliding_window.cls_pos)\n            sliding_window_pos.append(\n                [feat.sliding_window.sliding_window[i], feat.doc_id])\n            if len(feat.sliding_window.input_ids[i]) > MAX_LEN:\n                c += 1\n            if feat.sliding_window.sliding_window[i][0] == feat.sliding_window.sliding_window[i][1]:\n                print()\n            if i > 0 and feat.sliding_window.sliding_window[i][0] == feat.sliding_window.sliding_window[i-1][1]:\n                c += 1\n    input_ids = pad_sequences(input_ids,\n                              maxlen=MAX_LEN, value=0, padding=\"post\",\n                              dtype=\"long\", truncating=\"post\").tolist()\n    input_ids = torch.LongTensor(input_ids)\n    attention_masks = pad_sequences(attention_masks,\n                                    maxlen=MAX_LEN, value=0, padding=\"post\",\n                                    dtype=\"long\", truncating=\"post\").tolist()\n    attention_masks = torch.LongTensor(attention_masks)\n    subword_masks = pad_sequences(subword_masks,\n                                  maxlen=MAX_LEN, value=0, padding=\"post\",\n                                  dtype=\"long\", truncating=\"post\").tolist()\n    subword_masks = torch.LongTensor(subword_masks)\n    return SlidingWindowDatasetTest(input_ids, attention_masks, subword_masks, cls_pos, sliding_window_pos)\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-16T12:52:53.238296Z","iopub.execute_input":"2022-04-16T12:52:53.238559Z","iopub.status.idle":"2022-04-16T12:52:54.683379Z","shell.execute_reply.started":"2022-04-16T12:52:53.23853Z","shell.execute_reply":"2022-04-16T12:52:54.682675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoConfig, AdamW\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom torch.profiler import profile\nfrom torch.cuda.amp import autocast\nfrom tqdm import tqdm\nfrom sklearn.metrics import classification_report\n\nimport itertools\nimport random\nimport re\nimport os\nimport gc\nimport math","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:53:07.303374Z","iopub.execute_input":"2022-04-16T12:53:07.30364Z","iopub.status.idle":"2022-04-16T12:53:11.207885Z","shell.execute_reply.started":"2022-04-16T12:53:07.303612Z","shell.execute_reply":"2022-04-16T12:53:11.20717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if LONGBERT:\n    TOKENIZER = AutoTokenizer.from_pretrained(\"/kaggle/input/XXXXX/\") # waitfor uploading\nelse:\n    TOKENIZER = AutoTokenizer.from_pretrained(\"/kaggle/input/robertabase/\") \nTOKENIZER.add_special_tokens({'additional_special_tokens': ['[NP]']})","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:53:13.614226Z","iopub.execute_input":"2022-04-16T12:53:13.614576Z","iopub.status.idle":"2022-04-16T12:53:13.759353Z","shell.execute_reply.started":"2022-04-16T12:53:13.614537Z","shell.execute_reply":"2022-04-16T12:53:13.75823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def bound_to_matrix(bound: torch.Tensor) -> torch.LongTensor:\n    \"\"\"\n    To convert the boundary list to a adjacency matrix (like) that represents\n    the segment span.\n    1: start->end\n    2: end->start (or simply become 0 to ignore backward link)\n\n    input:\n        bound: [batch_size, seq_length]\n    \"\"\"\n    bs = bound.size(0)\n    mat = torch.zeros([bs, MAX_LEN, MAX_LEN], dtype=torch.long)\n    for b, seq in enumerate(bound):\n        for i, e in enumerate(seq):\n            if e == 1:\n                for j in range(i, MAX_LEN):\n                    if seq[j] == 2:\n                        mat[b][i][j] = 1\n                        break\n    return mat\n\nall_doc_ids = []\nall_doc_texts = []\nfor f in tqdm(list(os.listdir(TRAIN_PATH))):\n    all_doc_ids.append(f.replace('.txt', ''))\n    all_doc_texts.append(open(os.path.join(TRAIN_PATH, f),\n                         'r', encoding='utf-8').read())\n\ntest_doc_ids = []\ntest_doc_texts = []\nfor f in tqdm(list(os.listdir(TEST_PATH))):\n    test_doc_ids.append(f.replace('.txt', ''))\n    test_doc_texts.append(\n        open(os.path.join(TEST_PATH, f), 'r', encoding='utf-8').read())\n\nall_labels = pd.read_csv(TRAIN_LABEL)\n\ndef del_list_idx(l, id_to_del):\n    arr = np.array(l, dtype='int32')\n    return list(np.delete(arr, id_to_del))\n\n\nscope_len = len(all_doc_ids)\ntrain_len = math.floor((1 - TEST_SIZE - DEV_SIZE) * scope_len)\ndev_len = scope_len - train_len\nscope_index = list(range(scope_len))\ntrain_index = random.sample(scope_index, k=train_len)\n\ntrain_doc_ids = [all_doc_ids[i] for i in train_index]\ntrain_doc_texts = [all_doc_texts[i] for i in train_index]","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:53:15.992516Z","iopub.execute_input":"2022-04-16T12:53:15.993161Z","iopub.status.idle":"2022-04-16T12:54:41.073343Z","shell.execute_reply.started":"2022-04-16T12:53:15.993118Z","shell.execute_reply":"2022-04-16T12:54:41.072438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Dataset Loading...')\nif LONGBERT:\n    train_ds = torch.load('/kaggle/input/forwritingcompetition/longformer_train_ds.pt')\nelse:\n    train_ds = torch.load('/kaggle/input/forwritingcompetition/train_ds.pt')","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:58:08.134091Z","iopub.execute_input":"2022-04-16T12:58:08.134363Z","iopub.status.idle":"2022-04-16T12:58:10.88124Z","shell.execute_reply.started":"2022-04-16T12:58:08.134336Z","shell.execute_reply":"2022-04-16T12:58:10.880419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Create Dataset')\ndev_features = [DocFeature(doc_id=ids, raw_text=test_doc_texts[test_doc_ids.index(\n    ids)], train_or_test='test', tokenizer=TOKENIZER) for ids in test_doc_ids]\n\nif LONGBERT:\n    dev_ds = create_tensor_ds(dev_features)\n\nelse:\n    dev_ds = create_tensor_ds_sliding_window_test(dev_features)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:58:14.651421Z","iopub.execute_input":"2022-04-16T12:58:14.652167Z","iopub.status.idle":"2022-04-16T12:58:14.727361Z","shell.execute_reply.started":"2022-04-16T12:58:14.652117Z","shell.execute_reply":"2022-04-16T12:58:14.72663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sp = RandomSampler(train_ds)\ndev_sp = RandomSampler(dev_ds)\ndef custom_batch_collation(x):\n    num_elements = len(x[0])\n    return_tup = [[] for _ in range(num_elements)]\n    for row in x:\n        for i, e in enumerate(row):\n            return_tup[i].append(e)\n    return return_tup\n\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=train_sp, collate_fn=custom_batch_collation)\ndev_dl = DataLoader(dev_ds, batch_size=2, sampler=dev_sp, collate_fn=custom_batch_collation)\n\nif LONGBERT:\n    config = AutoConfig.from_pretrained(\"/kaggle/input/XXXXX/\")\nelse:\n    config = AutoConfig.from_pretrained(\"/kaggle/input/robertabase/\")\nconfig.num_labels = NUM_LABELS","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:58:21.938021Z","iopub.execute_input":"2022-04-16T12:58:21.938724Z","iopub.status.idle":"2022-04-16T12:58:21.948958Z","shell.execute_reply.started":"2022-04-16T12:58:21.938686Z","shell.execute_reply":"2022-04-16T12:58:21.948219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = TModel(config=config)\nmodel = model.to('cuda')\nmodel.transformer.resize_token_embeddings(len(TOKENIZER))\n\nbio_cls_weights = torch.Tensor([0, 100, 10, 100, 10, 100, 10, 100, 10, 100, 10, 100, 10, 100, 10, 5]).cuda()\nbio_loss = FocalLoss(ignore_index=0, gamma=2, alpha=bio_cls_weights)\nboundary_loss = FocalLoss(ignore_index=0, gamma=2, alpha=torch.Tensor([0,10,10,1]).cuda())\ntype_loss = FocalLoss(ignore_index=0, gamma=2)\nseg_loss = FocalLoss(ignore_index=0, gamma=2)\n\nbert_param_optimizer = list(model.transformer.named_parameters())\nner_fc_param_optimizer = list(model.ner.named_parameters())\nno_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n\nif not BASELINE:\n    boundary_param_optimizer = list(model.boundary.named_parameters())\n    type_param_optimizer = list(model.type_predict.named_parameters())\n\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in ner_fc_param_optimizer if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in ner_fc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in boundary_param_optimizer if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in boundary_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in type_param_optimizer if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in type_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n    ]\nelse:\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in ner_fc_param_optimizer if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01,\n            'lr': LEARNING_RATE},\n        {'params': [p for n, p in ner_fc_param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0,\n            'lr': LEARNING_RATE},\n    ]\nt_total = int(len(train_dl) / 1 * NUM_EPOCH)\noptimizer = AdamW(params=optimizer_grouped_parameters, lr=LEARNING_RATE)\n\n\nclass AverageMeter(object):\n    '''\n    computes and stores the average and current value\n    Example:\n        >>> loss = AverageMeter()\n        >>> for step,batch in enumerate(train_data):\n        >>>     pred = self.model(batch)\n        >>>     raw_loss = self.metrics(pred,target)\n        >>>     loss.update(raw_loss.item(),n = 1)\n        >>> cur_loss = loss.avg\n    '''\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:58:24.605198Z","iopub.execute_input":"2022-04-16T12:58:24.605452Z","iopub.status.idle":"2022-04-16T12:58:35.725659Z","shell.execute_reply.started":"2022-04-16T12:58:24.605415Z","shell.execute_reply":"2022-04-16T12:58:35.724955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"NUM_EPOCH","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:59:16.394843Z","iopub.execute_input":"2022-04-16T12:59:16.395341Z","iopub.status.idle":"2022-04-16T12:59:16.400367Z","shell.execute_reply.started":"2022-04-16T12:59:16.395303Z","shell.execute_reply":"2022-04-16T12:59:16.399494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(NUM_EPOCH):\n    torch.cuda.empty_cache()\n    model.train()\n    pbar = tqdm(total=len(train_dl), desc='Train')\n    train_loss = AverageMeter()\n    for step, batch in enumerate(train_dl):\n        bs = len(batch[0])\n        optimizer.zero_grad()\n        if LONGBERT:\n            input_ids, labels_type, labels_bio, labels_boundary, attention_masks, subword_masks = batch \n        else:\n            input_ids, labels_type, labels_bio, labels_boundary, attention_masks, subword_masks, cls_pos, sliding_window_pos = batch \n\n        input_ids = torch.stack(input_ids).cuda()\n        labels_type = torch.stack(labels_type).cuda()\n        labels_bio = torch.stack(labels_bio).cuda()\n        labels_boundary = torch.stack(labels_boundary).cuda()\n        attention_masks = torch.stack(attention_masks).cuda()\n        subword_masks = torch.stack(subword_masks).cuda()\n        active_padding_mask = attention_masks.view(-1) == 1\n\n        boundary_matrix = bound_to_matrix(labels_boundary).cuda()\n        pad_matrix = []\n        for i in range(bs):\n            tmp = attention_masks[i].clone()\n            tmp = tmp.view(MAX_LEN, 1)\n            tmp_t = tmp.transpose(0, 1)\n            mat = tmp * tmp_t\n            pad_matrix.append(mat)\n        pad_matrix = torch.stack(pad_matrix, 0)\n        matrix_padding_mask = pad_matrix.view(-1) == 1\n        with autocast():\n            #with torch.autograd.profiler.profile(use_cuda=True) as prof:\n            if BASELINE:\n                ner_logits = model(input_ids=input_ids, attention_mask=attention_masks)\n                ner_loss_ = bio_loss(\n                    ner_logits.view(-1, len(LABEL_BIO))[active_padding_mask], labels_bio.view(-1)[active_padding_mask])\n                loss = ner_loss_\n            else:\n                ner_logits, boundary_logits, type_logits, seg_logits = model(input_ids=input_ids, attention_mask=attention_masks)\n                ner_loss_ = bio_loss(\n                    ner_logits.view(-1, len(LABEL_BIO))[active_padding_mask], labels_bio.view(-1)[active_padding_mask])\n                boundary_loss_ = boundary_loss(boundary_logits.view(-1, len(BOUNDARY_LABEL))[active_padding_mask], labels_boundary.view(-1)[active_padding_mask])\n                type_loss_ = type_loss(type_logits.view(-1, len(LABEL_2_ID))[active_padding_mask], labels_type.view(-1)[active_padding_mask])\n                seg_loss_ = seg_loss(seg_logits.view(-1, len(BOUNDARY_LABEL_UNIDIRECTION))[matrix_padding_mask], boundary_matrix.view(-1)[matrix_padding_mask])\n                loss = ner_loss_+boundary_loss_+type_loss_+seg_loss_\n                    \n            #print(prof.key_averages().table())\n            loss.backward()\n            optimizer.step()\n        #torch.cuda.empty_cache()\n        #gc.collect()\n        train_loss.update(loss.item(), n=input_ids.size(0))\n        pbar.update()\n        pbar.set_postfix({'loss': train_loss.avg})\n    print(train_loss.avg)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T12:59:19.587345Z","iopub.execute_input":"2022-04-16T12:59:19.587776Z","iopub.status.idle":"2022-04-16T13:36:45.745514Z","shell.execute_reply.started":"2022-04-16T12:59:19.587741Z","shell.execute_reply":"2022-04-16T13:36:45.744823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BIO_LABEL={ 1:'Claim', 3:'Evidence', 5: 'Position', 7:'Concluding Statement',  9:'Lead', 11:'Counterclaim', 13: 'Rebuttal'}\n\ndef submit_formatting(ner_logits_i, subword_masks_i, text_id):\n    dataframe=pd.DataFrame()\n    positions=[]\n    labels=[]\n    end_prediction=subword_masks_i.max()\n    prev_e=None\n    prev_position=None\n    for i in range(len(ner_logits_i)):\n\n        e=ner_logits_i[i]\n        startposition=subword_masks_i[i]\n\n        if e % 2==1 and e!=15 and startposition!=-1 and e!=prev_e and startposition!=prev_position:\n            label= BIO_LABEL[e]\n            positions.append(startposition)\n            labels.append(label)\n            prev_e=e\n            prev_position=startposition\n    \n    positions.append(end_prediction)\n    positions=list(positions)\n\n    length=len(positions)\n    # print(positions)\n    if length==1:\n        list_=[]\n        dict1={}\n        dict1['id'] = text_id\n        dict1['class']= 'Evidence'\n        list_=[str(e) for e in range(0, positions[-1])]\n        dict1['predictionstring']=\" \".join(list_)\n        \n    else:\n        \n        pointer = 0\n        while pointer < length-1:\n            list_=[]\n            dict1={}\n            dict1['id'] = text_id\n            dict1['class']= labels[pointer]\n            list_=[str(e) for e in range(positions[pointer], positions[pointer+1])]\n            dict1['predictionstring']=\" \".join(list_)\n            # print(dict1)\n            dataframe=dataframe.append(dict1, ignore_index=True)\n            pointer+=1\n\n    return dataframe","metadata":{"execution":{"iopub.status.busy":"2022-04-16T13:56:27.199936Z","iopub.execute_input":"2022-04-16T13:56:27.20023Z","iopub.status.idle":"2022-04-16T13:56:27.212846Z","shell.execute_reply.started":"2022-04-16T13:56:27.200197Z","shell.execute_reply":"2022-04-16T13:56:27.211743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\nvalid_loss = AverageMeter()\npbar = tqdm(total=len(dev_dl), desc='Eval')\ndf_all=pd.DataFrame()\nfor batch in dev_dl:\n    bs = len(batch[0])\n    input_ids, attention_masks, subword_masks, cls_pos, sliding_window_pos = batch \n    input_ids = torch.stack(input_ids).cuda()\n    attention_masks = torch.stack(attention_masks).cuda()\n    subword_masks = torch.stack(subword_masks).cuda()\n    active_padding_mask = attention_masks.view(-1) == 1\n    \n    pad_matrix = []\n    for i in range(bs):\n        tmp = attention_masks[i].clone()\n        tmp = tmp.view(MAX_LEN, 1)\n        tmp_t = tmp.transpose(0, 1)\n        mat = tmp * tmp_t\n        pad_matrix.append(mat)\n    pad_matrix = torch.stack(pad_matrix, 0)\n    matrix_padding_mask = pad_matrix.view(-1) == 1\n\n    with torch.no_grad():\n        with autocast():\n            ner_logits, boundary_logits, type_logits, seg_logits = model(input_ids=input_ids, attention_mask=attention_masks)\n    \n    text_id=sliding_window_pos[0][1]\n    print(text_id)\n    attention_masks_i=attention_masks[0]\n    attention_masks_i=attention_masks_i==1\n    ner_logits_i=ner_logits.argmax(-1)[0]\n    ner_logits_i=ner_logits_i[attention_masks_i].cpu().numpy()\n    subword_masks_i=subword_masks[0]\n    subword_masks_i=subword_masks_i[attention_masks_i].cpu().numpy()\n    df=submit_formatting(ner_logits_i, subword_masks_i, text_id)\n    df_all=df_all.append(df,ignore_index=True)\n    try:\n        text_id=sliding_window_pos[1][1]\n        attention_masks_i=attention_masks[1]\n        attention_masks_i=attention_masks_i==1\n        ner_logits_i=ner_logits.argmax(-1)[1]\n        ner_logits_i=ner_logits_i[attention_masks_i].cpu().numpy()\n        subword_masks_i=subword_masks[1]\n        subword_masks_i=subword_masks_i[attention_masks_i].cpu().numpy()\n        df=submit_formatting(ner_logits_i, subword_masks_i, text_id)\n        df_all=df_all.append(df,ignore_index=True)\n    except:\n        pass","metadata":{"execution":{"iopub.status.busy":"2022-04-16T14:05:34.288165Z","iopub.execute_input":"2022-04-16T14:05:34.288458Z","iopub.status.idle":"2022-04-16T14:05:34.967539Z","shell.execute_reply.started":"2022-04-16T14:05:34.288418Z","shell.execute_reply":"2022-04-16T14:05:34.966757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all","metadata":{"execution":{"iopub.status.busy":"2022-04-16T14:05:42.41522Z","iopub.execute_input":"2022-04-16T14:05:42.415503Z","iopub.status.idle":"2022-04-16T14:05:42.426069Z","shell.execute_reply.started":"2022-04-16T14:05:42.415472Z","shell.execute_reply":"2022-04-16T14:05:42.425119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_all.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-16T14:06:00.449151Z","iopub.execute_input":"2022-04-16T14:06:00.449416Z","iopub.status.idle":"2022-04-16T14:06:00.459109Z","shell.execute_reply.started":"2022-04-16T14:06:00.449388Z","shell.execute_reply":"2022-04-16T14:06:00.458303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}